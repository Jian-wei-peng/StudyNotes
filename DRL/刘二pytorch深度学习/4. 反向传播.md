<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291419490.png" alt="image-20230629141914379" style="zoom: 33%;" />

对于上述简单的模型可以用解析式来做，但是对于复杂模型而言，如下图每个圆圈中都有一个权重，诶个写解析式求解极其麻烦，几乎不可能

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291423917.png" alt="image-20230629142305685" style="zoom:33%;" />

**计算图Computational Graph**

- 面对复杂网络，将其看作一个图，在图上来传播梯度，最终根据链式法则将其求出

- 以一个两层神经网络为例

  <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291513510.png" alt="image-20230629151310427" style="zoom: 33%;" />

- MM：Matrix Multiplication，矩阵乘法；ADD：向量加法

- 其中绿色模块就是计算模块

  <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291513355.png" alt="image-20230629151358261" style="zoom: 33%;" />

- **注意**：上面给出的神经网络$\hat{y}$可以对其进行展开，展开之后就会发现如果就这样一直线性展开，不管有多少层，最终得到的都能统一为一层，这样的话层数多和层数少都没区别

  <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291535702.png" alt="image-20230629153104333" style="zoom: 33%;" />

  - **解决方法：对每一层的输出加一个非线性函数，使其不能化简**

    <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291536046.png" alt="image-20230629153413641" style="zoom:33%;" />

---

**链式法则（Chain Rule）**：

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291538559.png" alt="image-20230629153810448" style="zoom:33%;" />

步骤：

1. **Create Computational Graph（Forward）**

   - 前馈运算
     - 从输入$x$沿着边向最终的loss计算

   <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291558804.png" alt="image-20230629155842728" style="zoom:33%;" />

2. Local Gradient

   - 函数$f$是用于计算输出$Z$关于输入$x$和权重$w$的导数
   - $Z = f(x,w)$

   <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291605121.png" alt="image-20230629160555055" style="zoom:33%;" />

3. **Given gradient from successive node**

   - 对于输出结果$Z$而言，首先要拿到最终的损失函数Loss对它的偏导$\frac{\partial L}{\partial z}$
     - 该偏导是从Loss传回来的。先是从最初的输入$x$通过前馈一步一步计算到最终的损失函数Loss（前馈过程），然后再从Loss开头一步一步往回算（反馈过程）

   <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291613893.png" alt="image-20230629161338821" style="zoom:33%;" />

4. **Use chain rule to compute the gradient (Backward) **

   - 拿到$\frac{\partial L}{\partial z}$后，经过计算$f$，我们的目标是得到Loss关于输入$x$和权重$w$的偏导，这一计算过程需使用上链式法则

   <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291617589.png" alt="image-20230629161731489" style="zoom:33%;" />

实例一：

- $f = x · w$，令$x = 2, w = 3$
  - 输出Z关于输入$x$的偏导：$\frac{\partial Z}{\partial x} = \frac{\partial x·w}{\partial x} = w$
  - 输出Z关于权重$w$的偏导：$\frac{\partial Z}{\partial w} = \frac{\partial x·w}{\partial w} = x$

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291618265.png" alt="image-20230629161856196" style="zoom:33%;" />

前馈过程：

- 由$x$和$w$计算得到$z$

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291628837.png" alt="image-20230629162015774" style="zoom:33%;" />

反馈过程：

- 假设由前一步得到最终的损失函数Loss对$Z$的偏导为5

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291625152.png" alt="image-20230629162559054" style="zoom:33%;" />

---

线性模型$\hat{y} = x * w$的计算图

- 模型计算图

  - 假设输入$x = 1$，权重$w = 1$，那么$\hat{y} = x * w = 1$

  <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291639631.png" alt="image-20230629163941595" style="zoom:35%;" />

- loss计算图，$loss = (\hat{y} - y)^2 = (x·w - y)^2$

  - **将$(\hat{y} - y)$称为残差项（residual）**，记为$r = \hat{y} - y$

  <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291647968.png" alt="image-20230629164743921" style="zoom:37%;" />

  - 假设$y = 2$，那么残差项$r = \hat{y} - y = -1$，残差对$\hat{y}$的导数$\frac{\partial \hat{y} - y}{\partial \hat{y}} = 1$

上述即为前馈过程：

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291649717.png" alt="image-20230629164931663" style="zoom:37%;" />

- 不仅能计算出到下一步的输出值，还能够计算出局部的梯度

接下去就是反馈过程：

- 首先求损失函数loss关于残差项$r$的偏导，$loss = r^2$

  <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291652295.png" alt="image-20230629165221255" style="zoom:37%;" />

- 再计算损失关于$\hat{y}$的偏导

  <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291653146.png" alt="image-20230629165314090" style="zoom:37%;" />

- 最终得到损失关于权重$w$的偏导

  <img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291654084.png" alt="image-20230629165406034" style="zoom:33%;" />

完整的计算图：

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291654037.png" alt="image-20230629165438941" style="zoom:40%;" />

---

**PyTorch中如何实现前馈和反馈计算？？？**

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306291657072.png" alt="image-20230629165755950" style="zoom: 33%;" />

```python
import torch    # 导入pytorch库

# 训练集
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

# 权重
w = torch.Tensor([1.0])     # 使用pytorch中的Tensor进行定义赋值
w.requires_grad = True      # 表示需要计算梯度，默认的是False，即不计算梯度


def forward(x):
    # 定义模型：y_hat = x * w，其中w是一个张量Tensor，因此该乘法*被重载了，变成了Tensor之间的数乘
    # x需要为Tensor，如果不是，则会自动转换成Tensor
    return x * w


def loss(x, y):
    # 定义损失函数loss function
    y_pred = forward(x)
    return (y_pred - y) ** 2   # (y_hat - y)^2


print('Predict (before training)', 4, forward(4).item())

for epoch in range(100):
    for x, y in zip(x_data, y_data):
        l = loss(x, y)  # 计算loss，结果是Tensor（前馈）
        l.backward()    # 反馈，l是Tensor
        print("\tgrad: ", x, y, w.grad.item())  # item是将梯度中的数值取出来作为一个标量
        # w是一个张量，包含data和grad，其中grad也是张量，因此是要取grad的数据data
        w.data = w.data - 0.01 * w.grad.data
        # .data是Tensor操作，.item()是将数值取出来当成标量使用

        w.grad.data.zero_()  # 更新完成后将梯度清零，否则会被累加到下一轮训练

    print("progress: ", epoch, l.item())

print('Predict (after training)', 4, forward(4).item())
```

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306292037179.png" alt="image-20230629203721119" style="zoom:33%;" />

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306292037179.png" alt="image-20230629203721119" style="zoom:33%;" />

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306292019387.png" alt="image-20230629201909191" style="zoom: 25%;" />

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202306292038871.png" alt="image-20230629203806720" style="zoom:30%;" />









